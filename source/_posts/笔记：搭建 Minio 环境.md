---
title: 笔记：搭建 Minio 环境
date: 2025-07-08
categories: 
tags: []
author: 霸天
layout: post
---
## 1. 单机测试环境

### 1.1. 创建宿主机数据挂载目录

```
mkdir -p /mystudy/data/minio
```

---


### 1.2. 启动 Minio 容器

```
docker run -d \
  --name minio-test \
  -p 9000:9000 \
  -p 9001:9001 \
  -e "MINIO_ROOT_USER=admin" \
  -e "MINIO_ROOT_PASSWORD=password" \
  -v /mystudy/data/minio:/data \
  minio/minio:RELEASE.2025-06-13T11-33-47Z \
  server /data --console-address ":9001"
```

> [!NOTE] 注意事项
> 1. 密码长度至少 8 个字符
> 2. `server /data --console-address ":9001"` 是 MinIO 启动命令中的一个子命令，用来启动 MinIO 对象存储服务的：
> 	1. server：
> 		1. 启动 MinIO 存储服务
> 	2. /data
> 		1. MinIO 存储数据的根目录
> 	3. --console-address ":9001"
> 		1. 启动控制台 Web UI，监听 9001 端口
> 3. 9000 端口是 Minio 端口，9001 端口是 Web UI 端口
> 4. 会话级别以太网临时代理：
```
export http_proxy="http://172.20.10.3:7890" && export https_proxy="http://172.20.10.3:7890" && export no_proxy="localhost,127.0.0.1,.svc,.cluster.local,192.168.136.0/24,10.96.0.1,10.244.0.0/16" && export HTTP_PROXY=$http_proxy && export HTTPS_PROXY=$https_proxy && export NO_PROXY=$no_proxy
```

---

### 1.3. 访问 Minio Web UI
 
访问： http://192.168.136.8:9001

----


## 2. Minio 分布式集群

### 环境特性

#### 数据写入

MinIO 的数据写入具备强一致性，具体表现如下：

关于一些元数据操作，例如创建桶、添加标签、用户及用户组管理、对象多部分上传的临时元数据等，通常都会经过如下流程：
1. 客户端通过 API 或命令行发起元数据相关请求。
2. 集群中任意节点作为入口节点接收该请求。
3. 入口节点执行用户身份验证和权限授权检查，通常包括桶策略和用户策略两部分。
4. 然后入口节点解析请求，确定需要操作的桶、对象或配置，生成对应的元数据内容（如 JSON、配置文件、标签文本等）
5. 入口节点将该元数据变更广播至所有相关节点，所有参与节点在本地存储路径（通常位于 `.minio.sys` 目录结构内）同步阻塞地写入或更新相应元数据文件，确保一致性。
6. 当所有节点写入成功后，入口节点返回操作成功响应（如 HTTP 200）。
7. 若任一节点写入失败，整体操作回滚，入口节点返回错误信息（如 HTTP 500）。

而对于对象上传，MinIO 会采用纠删码机制，通常流程如下：
1. 客户端通过 API 或命令行发起上传对象请求。
2. 集群中任意节点作为入口节点接收该请求。
3. 入口节点执行用户身份验证和权限授权检查，通常包括桶策略和用户策略两部分。
4. 入口节点根据对象名和桶名，通过一致性哈希计算确定对象所属的 Erasure Set，并定位所需分片所在的节点和磁盘；
5. 入口将对象数据切分为多个数据分片，经过纠删码编码生成校验分片（例如 12 个数据分片 + 4 个校验分片），并行发送给对应节点。
6. 各节点在本地磁盘写入自己的分片数据，同时生成并存储对应的元数据（如对象大小、ETag、时间戳等），入口节点在此期间等待所有参与节点确认写入成功。
7. 所有数据分片和校验分片成功写入后，入口节点向客户端返回成功响应（如 HTTP 200）。
8. 若任一分片写入失败，整体上传操作回滚，入口节点返回错误信息（如 HTTP 500）。

> [!NOTE] 注意事项
> 1. 对象级元数据（如 xl.meta）仅写入其所属的 Erasure Set（ES）组内节点，不会同步到其它 ES；
> 2. 而系统级元数据（如桶配置、策略、用户信息等）则会**同步写入集群中所有节点**，确保全局一致性。

----


#### 数据读取

关于元数据的读取，通常流程如下：
1. 客户端通过 API 或命令行发起读取元数据请求
2. 集群中任意节点作为入口节点接收该请求
3. 入口节点执行用户身份验证和权限授权检查，通常包括桶策略和用户策略两部分
4. 然后入口节点在自己的 `.minio.sys/` 路径下查找对应元数据文件
	1. 正常情况下，每个节点都有完整的元数据副本，可以直接读取本地即可
	2. 若本地副本缺失或读取失败，入口节点可能尝试从其他节点同步获取
5. 入口节点将读取到的元数据封装为响应内容，返回给客户端（如 JSON）。

而对于对象下载，通常流程如下：
1. 客户端通过 API 、命令行，也可能是通过预签名 URL 发起对象下载请求
2. 集群中任意节点作为入口节点接收该请求
3. 入口节点执行用户身份验证和权限授权检查，通常包括桶策略和用户策略两部分
4. 入口节点根据对象名和桶名，通过一致性哈希计算确定对象所属的 Erasure Set，并定位所需分片所在的节点和磁盘；同时读取对象元数据（如 ETag、大小、版本等）
5. 以 EC(12,4) 为例，入口节点优先从计算得到的 12 个数据分片所在节点并发拉取数据分片；
6. 入口节点会对每个拉取到的分片执行大小和哈希校验，确认分片完整有效。
7. 如果部分数据分片不可用或校验失败，会启用冗余容错机制，从校验分片节点补拉，确保至少获得 12 个有效分片。
8. 使用获得的任意 12 个有效分片（数据+校验混合）一同通过 Reed-Solomon 解码恢复对象数据（如果 12 个都是数据分片，就无需解码了）
9. 对解码恢复后的完整对象进行哈希校验（如 ETag 比对），确保数据完整无误后，通过 HTTP 流形式，将完整对象数据返回给客户端。
10. 若可用分片不足 12 个或校验失败，返回相应错误码（如 503 或 500）。

----


#### 容灾冗余

##### 纠删码概述

MinIO 使用纠删码（Erasure Code）实现容灾冗余。简单来说，就是将对象拆分为 K 个数据分片，同时生成 m 个校验分片。假设 s = k + m，在这 s 个分片中，任意选取 k 个（无论是数据分片还是校验分片）即可恢复出原始数据。换句话说，即使任意 m 个分片损坏，系统仍然能够完整还原原始数据。
![](image-20250716170557252.png)

EC 存储的优点是：
1. 磁盘利用率高：
	1. 相比于 3 副本机制，EC 仅需额外存储少量校验片即可实现容错，有效降低冗余数据占比。
	2. 例如：EC(k=12, m=4) 可容忍任意 4 块磁盘故障，但冗余开销仅为 33%；而 3 副本机制的冗余开销高达 200%。
2. 容错能力高：
	1. 在 EC(k, m) 模型中，任意丢失 m 个分片都能恢复原始数据，适合应对多节点或多磁盘同时故障的分布式场景。
3. 天然支持数据校验和完整性验证：
	1. 纠删码通过数学方式生成校验片，本身具备数据校验和完整性验证能力，无需额外机制支持。

EC 存储的缺点是：
1. 写入和恢复开销较大：
	1. 写入时需先分片再计算校验片，恢复时需解码，整个编解码过程会消耗大量 CPU 与网络带宽，开销主要体现在写入操作和故障恢复读取时
2. 故障读取时延迟高于副本机制：
	1. 正常读取时延迟较低，但当部分分片不可用时，需从多个节点获取数据并进行解码，导致延迟明显增加，尤其在节点故障情况下更为显著。
3. 实现的复杂度高：
	1. EC 的编码与解码涉及较为复杂的线性代数运算，实现难度较大，对开发和系统维护要求更高。
4. 对网络与时间同步依赖强：
	1. 多个分片需并行写入与恢复，依赖较低延迟和较高一致性的网络环境，在跨地域部署时，网络抖动或时间漂移可能影响可靠性和性能

因此，EC 存储更适用于冷数据归档、大文件存储以及对数据可靠性要求高的场景；而对于低延迟访问、频繁读写或高并发写入等场景，则并不适合。

> [!NOTE] 注意事项：MinIO 为什么在 EC 存储有这些短板的情况下，依然成为对象存储界的一哥
> 1. 定位精准，专注对象存储：
> 	1. 它不像 Ceph 走“全能型”路线（块、文件、对象都搞），MinIO 从一开始就专注于 S3 兼容对象存储，在接口、语义和生态适配上几乎与 AWS S3 完美对齐，天然契合云原生环境。
> 2. 纠删码 + 分布式架构 = 面向未来的弹性扩展：
> 	1. 虽然 EC 在写入方面有延迟劣势，但带来了更高的容量利用率、更强的容错能力和更低的存储成本，成为现代大规模对象存储架构的基础。
> 3. 天然云原生，完美适配 Kubernetes：
> 	1. MinIO 是最早深度集成 Kubernetes 的对象存储之一，原生支持 Operator、Helm、StatefulSet 等，轻松融入 DevOps 和云原生部署体系
> 4. 大多数情况下是 ”正常读“，不会触发重建：
> 	1. 虽然故障读取时需要进行重建，耗时较长，但在分布式存储环境下，只要网络和磁盘状态良好，大部分读操作都能直接从原始数据片获取，无需解码进行重建，延迟较低。
> 	2. 因此，性能瓶颈主要聚焦在写入环节。
> 5. EC 的 “写入短板” 在对象存储主流场景中并不致命
> 	1. 对象存储主要面向大文件和非结构化数据（如图像、视频、日志、备份等），通常写少读多，对 EC 架构更为友好
> 	2. 云原生工作负载更关注容量、可用性与成本，而非毫秒级访问延迟。
> 	3. MinIO 对 EC 实现做了大量优化，如预分配编码缓冲区、并发 IO 管理等，有效降低了编码带来的性能影响

----


##### 纠删码参数

MinIO 在首次启动集群时，会根据所提供的磁盘数量自动推导出默认的 EC 参数（k 和 m），并将该参数写入每个磁盘上的 `.minio.sys` 元数据目录中。此后集群运行将固定使用该参数组合，无法变更。

在某些特定版本的 MinIO 中，也支持在首次启动时通过 `--erasure-coding.k` 和 `--erasure-coding.m` 手动指定 k 和 m。需要注意的是，此方式下 k + m 一般不得超过 16，且总磁盘数量必须是 k + m 的整数倍，且不得少于该值。

----



部分ok，恢复怎么恢复










































### 2.1. 环境要求

#### 2.1.1. 节点要求

官方建议 MinIO 分布式集群至少部署 4 个节点。这是因为纠删码模式运行时最少需要 4 块磁盘才能生效，而如果只提供 4 块磁盘，那么需要每个节点都挂载一块磁盘（不推荐采用一个节点挂载 4 块磁盘，或两个节点共计 4 块磁盘的方式部署，因为这种架构下单点故障风险较高，无法实现真正意义上的数据高可用与分布式冗余）

节点数量是奇数还是偶数并不关键，关键在于保证最终的磁盘总数为偶数。例如，7 个节点每个挂载 2 个磁盘，总磁盘数为 14 个。通常推荐使用偶数个节点，主要是基于每个节点仅有一块磁盘的情况，但我们一般都是部署偶数个节点。

在早期版本中，MinIO 集群中建议最多使用不超过 32 块磁盘。即使每个节点仅挂载一块磁盘，那么集群也最多不能超过 32 个节点。

MinIO 集群中，虽然节点数量的增加有助于提升容量和并发性能，但同时也带来一系列挑战与潜在问题：
1. 元数据一致性成本上升：
	1. MinIO 采用去中心化架构，所有节点共同管理元数据（如 bucket 配置、版本控制、对象锁等）；
		1. 元数据由节点负责，而非磁盘维护，相关信息以 Key-Value 形式存储于每个节点本地的 `.minio.sys` 目录中。
	2. 随着节点增多，元数据的广播与同步耗时显著增加，尤其在网络存在延迟或抖动时更为明显。
	3. 节点数量增加，网络负载成倍上升，易形成“网状拓扑”瓶颈，影响集群整体性能。
2. 运维成本提升：
	1. 节点越多，需监控的服务日志、性能指标与健康状态也随之增多，运维复杂度提升。
3. 故障排查难度加大：
	1. 一次写入失败，可能涉及多个节点的 I/O、网络或锁状态，排查路径长，定位难度显著上升。
4. 启动与恢复耗时增长：
	1. MinIO 启动时需要加载全部节点信息、初始化纠删码映射、执行健康检查
	2. 节点越多，启动时间越长，若某些节点响应缓慢，可能导致整个集群初始化受阻或卡顿。

在新版本中，MinIO 已取消对最大磁盘数 32 的限制，单个集群可支持无限数量的磁盘。也就是说，单个集群可支持无限数量的节点，但出于性能、稳定性和运维复杂度的综合考虑，仍推荐将节点数控制在 32 个以内。

一般来说，4 到 8 个节点属于中型部署规模，10 到 16 个节点则适用于高并发、大容量的场景，而超过 16 个节点的部署则属于超大规模集群，常见于企业级云平台、公有云提供商。

当需要构建大规模分布式存储系统时，不建议持续扩展单个 MinIO 集群的节点数量，而是推荐通过部署多个独立的 MinIO 集群，并借助分布式协调机制，将多个集群组成联邦架构，实现更灵活的横向扩展与统一管理。

> [!NOTE] 注意事项
> 1. 推荐每个节点仅运行一个 Minio 实例，以确保集群稳定性

---


#### 2.1.2. 磁盘要求

MinIO 要求至少使用 4 块磁盘，这是因为纠删码模式在运行时至少需要 4 块磁盘才能正常工作（EC:2, 2）。推荐每台机器挂载 2 块磁盘，不建议超过 4 块磁盘。

推荐每台机器挂在的磁盘数量一致，并且总磁盘数为偶数个，这是因为偶数数量更有利于提升纠删码的存储效率、容错能力与扩展时的平衡性，这一经验来自实际部署中的性能表现。

在早期版本中，MinIO 集群建议最多使用不超过 32 块磁盘。新版本已取消对最大磁盘数 32 的限制，单个集群可支持无限数量的磁盘，但仍建议控制在 32 块以内。

> [!NOTE] 注意事项
> 1. 文件系统推荐使用 XFS 或 ext4

---


#### 2.1.3. CPU 要求

纠删码计算和数据加密对 CPU 负载较大，CPU 性能直接影响整体吞吐和响应延迟。建议至少配置 4 核 CPU 起步，以更好地支持并发请求及加密计算（如 TLS/SSL 加解密和纠删码编码/解码）。

一般而言，小型或中型集群以 4 核 CPU 起步，大型高并发集群则建议配备 8 核或以上的 CPU。

---


#### 2.1.4. 内存要求

较大内存有助于提升整体性能和响应速度，尤其在高负载场景下效果更为明显。一般而言，小型或中型集群以 8GB 内存起步，大型高并发集群则建议配备 16GB 或以上的内存。

如果启用了大量加密（如 SSE-C、SSE-KMS）或复杂的纠删码配置，建议适当增加内存；此外，在使用 Kubernetes Operator 或多租户环境时，内存需求也会相应提升。

---


#### 2.1.5. 网络要求

一般而言，小型或中型集群以千兆（1 Gbps）网络带宽起步，大型高并发集群则建议配备 10 Gbps 或以上的网络带宽。

单个集群中的所有节点必须部署在同一局域网内，确保低延迟、低丢包率。不建议跨机房部署同一个集群，因为网络延迟和不确定性将严重影响纠删码的数据一致性与性能。

---


#### 2.1.6. 时间要求

MinIO 分布式集群中，各节点之间必须保持时间同步。

> [!NOTE] 注意事项
> 1. 在跨多集群时，时间同步更加敏感，必须进行时间同步。

---


#### 2.1.7. 用户要求

在生产环境中，建议新建一个专门的系统用户（例如命名为 `minio`）来运行 MinIO 服务，不建议赋予其过高权限（如 root 权限），以降低安全风险。通过限制权限，可以有效防止因服务漏洞导致整个系统被攻破。

该用户需对存储数据目录具备读写权限，并能够访问 MinIO 监听的端口。

----


#### 2.1.8. 端口要求

| 端口   | 类型  | 说明        |
| ---- | --- | --------- |
| 9000 | TCP | HTTP 服务端口 |
| 9001 | TCP | Web 控制台端口 |

----


#### 2.1.9. 扩容要求

扩容时应尽量保证最终磁盘总数为偶数，并且由于纠删码参数（EC 的 k 和 m 值）在集群初始化时便已固定，无法后期更改，因此磁盘总数应为 EC 分组大小的整数倍。

例如，若采用 EC(4,2)，每个 Erasure Set 由 6 块磁盘组成，那么在这 6 块磁盘中已经写入 `.minio.sys` 的情况下，合理的扩容目标应为 12、18、24、30 等磁盘数。

假如我们将磁盘扩展至 12 块，此时已有的 6 块磁盘中存在 `.minio.sys`，而新增的 6 块磁盘为空盘，尚未初始化。MinIO 在启动时会扫描这 6 块新盘，发现它们均不含 `.minio.sys`，且数量刚好满足一个完整 Erasure Set 的要求，于是会自动将其初始化为新的 ES，并写入对应新的 `.minio.sys`。

最终，集群中将包含 2 个 Erasure Set，每组的 stripe size 为 6。各 ES 拥有独立的元数据，但共享身份认证配置和 API 接口。此后新写入的数据将由 MinIO 在多个 ES 间选择其一进行存储。

而若扩容至 10 块磁盘这类非整倍数的情况，MinIO 将因无法形成完整的 Erasure Set 而拒绝启动。

如果确有修改 EC 参数的需求，我们通常有以下三种策略：
1. 删除原集群中的 `.minio.sys` 和所有数据，重新启动集群。此时 MinIO 会将其视为一个全新的集群，并根据当前磁盘数量重新推导 EC 参数
2. 保留原集群，另行搭建一个新集群，将原集群中的数据迁移至新集群，随后逐步下线旧集群。
3. 采用联邦架构策略：保留现有 EC(4,2) 的集群，同时部署一个新的集群（如 EC(12,4)），并通过 MinIO Gateway 或联合访问机制，将多个集群整合为一个逻辑上的统一存储空间。

> [!NOTE] 注意事项
> 1. 当需要构建大规模分布式存储系统时，不建议持续扩展单个 MinIO 集群的节点数量，而是推荐通过部署多个独立的 MinIO 集群，并借助分布式协调机制，将多个集群组成联邦架构，实现更灵活的横向扩展与统一管理。

---


#### 2.1.10. Swap 分区要求

swap 分区，简单来说，是操作系统在系统内存不足时，将某些程序不活跃或暂时不需要的数据移动到 swap 分区（位于硬盘），以释放内存资源供当前活跃程序使用。

由于 swap 分区依赖硬盘，其读写速度远低于内存。如果系统频繁使用 swap，会造成大量磁盘 I/O，严重影响系统性能，甚至导致系统响应迟缓或无响应。因此，在性能敏感的场景（如数据库、实时计算、大内存服务器）中，通常建议禁用 swap，确保数据始终在高速内存中运行。

我们 MinIO 要求关闭 swap 分区。

----


### 2.2. 环境准备

#### 2.2.1. Ubuntu 版本（22.04）

```
lsb_release -a
```

----


#### 2.2.2. 节点列表

| IP             | 磁盘                 | CPU | 内存  |
| -------------- | ------------------ | --- | --- |
| 192.168.136.8  | Root 盘、四块 SCSI 数据盘 | 2 核 | 4GB |
| 192.168.136.9  | Root 盘、四块 SCSI 数据盘 | 2 核 | 4GB |
| 192.168.136.10 | Root 盘、四块 SCSI 数据盘 | 2 核 | 4GB |
| 192.168.136.11 | Root 盘、四块 SCSI 数据盘 | 2 核 | 4GB |

-----


#### 2.2.3. 时间同步

```
command -v chrony >/dev/null 2>&1 || sudo apt-get install -y chrony


sudo systemctl enable chrony && sudo systemctl start chrony
```

> [!NOTE] 注意事项
> 1. 会话级别以太网临时代理：
```
export http_proxy="http://172.20.10.3:7890" && export https_proxy="http://172.20.10.3:7890" && export no_proxy="localhost,127.0.0.1,.svc,.cluster.local,192.168.136.0/24,10.96.0.1,10.244.0.0/16" && export HTTP_PROXY=$http_proxy && export HTTPS_PROXY=$https_proxy && export NO_PROXY=$no_proxy
```

----


#### 2.2.4. 开放端口

```
sudo ufw allow 9000/tcp


sudo ufw allow 9001/tcp
```

> [!NOTE] 注意事项
> 1. 如果使用云服务器，同样需要开放安全组

---


#### 2.2.5. 关闭 Swap 分区

```
# 1. 编辑 /etc/fstab
vim /etc/fstab
"""
# 将此内容进行注释（/swap 开头的）
# /swap.img       none    swap    sw      0       0
"""


# 2. 立即关闭 Swap 分区
swapoff -a
```

----


#### 2.2.6. 安装需要的工具

```
// 1. dos2unix，用于将文件转为 Unix 格式
command -v dos2unix >/dev/null 2>&1 || sudo apt-get install -y dos2unix


// 2. xfsprogs，用于将磁盘块设备格式化为 xfs 文件系统
command -v xfsprogs >/dev/null 2>&1 || sudo apt install -y xfsprogs
```

> [!NOTE] 注意事项
> 1. 会话级别以太网临时代理：
```
export http_proxy="http://172.20.10.3:7890" && export https_proxy="http://172.20.10.3:7890" && export no_proxy="localhost,127.0.0.1,.svc,.cluster.local,192.168.136.0/24,10.96.0.1,10.244.0.0/16" && export HTTP_PROXY=$http_proxy && export HTTPS_PROXY=$https_proxy && export NO_PROXY=$no_proxy
```

----


### 2.3. 环境搭建

#### 2.3.1. 创建软件安装路径

```
mkdir -p /mystudy/minio
"""
1. -p：
	1. 如果 /mystudy 目录本身不存在，可以加上 -p 参数，递归创建所有父目录
"""
```

----


#### 2.3.2. 安装 Minio Server

```
cd /mystudy/minio
```

接着参考 [MinIO 官方下载页面](https://min.io/open-source/download?platform=linux) 进行安装，安装过程会将 `minio` 文件下载到当前目录，文件名即为 `minio`（没有扩展名）。

需要注意的是，此处下载的是 MinIO Server，以下是 MinIO 的几个组件区别说明：
1. MinIO Server：
	1. 用于部署 MinIO 服务端时下载，用来搭建对象存储系统。
2. MinIO Client：
	1. 一款命令行工具，用于管理 MinIO 或兼容的 AWS S3 服务，支持上传、下载、创建桶、设置权限等操作。
	2. 如果你希望通过命令行操作 MinIO 或 AWS S3，就需要安装它！
3. MinIO SDK
	1. 面向开发者，用于在代码中操作 MinIO（如上传、下载、授权等），需根据所用编程语言选择对应版本（如 Java、Python、Go）
![](image-20250715220301330.png)

> [!NOTE] 注意事项
> 1. 安装 amd64 还是 arm64，要看系统 CPU 架构
```
uname -m
“”“
1. x86_64：
	1. amd64
2. aarch64：
	1. arm64
”“”
```

----


#### 2.3.3. 磁盘格式化与挂载

```
# 1. 创建磁盘挂载目录
mkdir -p /mystudy/minio/disk{1..4}


# 2. 列出所有块设备
lsblk


# 3. 格式化块（需要格式化为 XFS 或 ext4），这里格式化为 XFS
mkfs.xfs /dev/sdb

mkfs.xfs /dev/sdc

mkfs.xfs /dev/sdd

mkfs.xfs /dev/sde


# 4. 将磁盘挂载到挂载目录
mount /dev/sdb /mystudy/minio/disk1

mount /dev/sdc /mystudy/minio/disk2

mount /dev/sdd /mystudy/minio/disk3

mount /dev/sde /mystudy/minio/disk4


# 5. 再次列出所有块设备，查看是否挂载成功
lsblk
```

----


#### 2.3.4. 编写 Shell 启动脚本

```
# 1. 进入 Minio 目录
cd /mystudy/minio


# 2. 创建 Shell 启动脚本
vim start-minio.sh
"""
#!/bin/bash

# 设置根账户和密码
export MINIO_ROOT_USER=admin
export MINIO_ROOT_PASSWORD=password

# 多机分布式模式
nohup /mystudy/minio/minio server \
  --config-dir /etc/minio \
  --address :9000 \
  --console-address :9001 \
  http://192.168.136.8/mystudy/minio/disk{1..4} \
  http://192.168.136.9/mystudy/minio/disk{1..4} \
  http://192.168.136.10/mystudy/minio/disk{1..4} \
  http://192.168.136.11/mystudy/minio/disk{1..4} \
  > /var/log/minio.log 2>&1 &

echo "MinIO 多机分布式集群已启动，后台运行…"

"""

// 2. 将脚本转为 Unix 格式
dos2unix /mystudy/minio/start-minio.sh


# 3. 添加可执行权限
chmod +x /mystudy/minio/start-minio.sh
```

> [!NOTE] 注意事项
> 1. Shell 启动脚本中的最后一个空白行并非笔误，它是必须添加的内容。
> 2. 我们可以在 MinIO 首次启动时手动指定纠删码参数（EC 的 k 和 m）。此时，所提供的磁盘数量必须是 k + m 的整数倍，且不得少于 k + m，否则将导致启动失败。
> 3. 需要注意的是，k + m 一般不超过 16，并且能手动指定纠删码参数的，只有某些特定 Minio 版本可以使用
```
#!/bin/bash

# 设置根账户和密码
export MINIO_ROOT_USER=admin
export MINIO_ROOT_PASSWORD=password

# 多机分布式模式，指定纠删码参数为 EC(10,6)
nohup /mystudy/minio/minio server \
  --config-dir /etc/minio \
  --address :9000 \
  --console-address :9001 \
  --erasure-coding.k 10 \
  --erasure-coding.m 6 \
  http://192.168.136.8/mystudy/minio/disk{1..4} \
  http://192.168.136.9/mystudy/minio/disk{1..4} \
  http://192.168.136.10/mystudy/minio/disk{1..4} \
  http://192.168.136.11/mystudy/minio/disk{1..4} \
  > /var/log/minio.log 2>&1 &

echo "MinIO 多机分布式集群已启动，后台运行…"

```

----


#### 2.3.5. 启动分布式集群

```
/mystudy/minio/start-minio.sh


ps aux | grep minio
```

> [!NOTE] 注意事项
> 1. 每个节点都要启动

----






















